---
title: Streaming HLS
---

D√©j√† 2 ans sans article de blog‚Ä¶ On va corriger √ßa du coup !

Aujourd‚Äôhui, on va parler streaming, et surtout comment en faire avec (presque) que du logiciel libre et en tout cas en √©vitant au maximum les outils habituels privateurs et en proposant des alternatives auto-h√©bergeables et d√©centralis√©es pour tous !

# Les probl√®mes du streaming ¬´¬†standard¬†¬ª

Le streaming ¬´¬†√† l‚Äôancienne¬†¬ª est g√©n√©ralement bas√© sur un flux TCP continu, type [RTP](https://fr.wikipedia.org/wiki/Real-time_Transport_Protocol), [RTSP](https://fr.wikipedia.org/wiki/Real_Time_Streaming_Protocol) ou [RTMP](https://fr.wikipedia.org/wiki/Real_Time_Messaging_Protocol).

Ces formats fonctionnent plut√¥t bien tant qu‚Äôon a √† disposition du mat√©riel professionnel de diffusion, mais c‚Äôest g√©n√©ralement la croix et la banni√®re quand on veut faire avec du mat√©riel classique comme on en a partout √† la maison.

Le principal probl√®me de ce type de flux est qu‚Äôil s‚Äôagit justement d‚Äôun flux¬†! Il faut une connexion permanente et stable entre le lieu de la captation et le serveur de streaming. Au moindre glitch r√©seau, g√©n√©ralement tout plante et il faut tout relancer, et dans le bon ordre.
√áa suppose aussi des choses assez exotiques, type du [multicast](https://fr.wikipedia.org/wiki/Multicast) comme on en trouve dans [la documentation officielle](https://trac.ffmpeg.org/wiki/StreamingGuide) de [ffmpeg](https://ffmpeg.org/), qui n√©cessitent en plus du mat√©riel r√©seau compatible.
Idem c√¥t√© client, chaque client va devoir maintenir un flux constant ouvert, ce qui engorge rapidement le serveur. Et parler de cache ou de miroir sur du flux rel√®ve bien entendu de l‚Äôexploit voire de l‚Äôimpossibilit√©‚Ä¶ 

L‚Äôautre probl√®me, c‚Äôest qu‚Äôon trouve tout plein de solutions qui reposent sur un encodage des vid√©os au niveau du serveur en face et non de l√† o√π est effectu√©e la captation. C‚Äôest par exemple le cas avec [le module RTMP pour Nginx](https://github.com/arut/nginx-rtmp-module), qui prend un unique flux en entr√©e (g√©n√©ralement du 1080p) et le transcode dans les autres r√©solutions (720p, 480p, 320p, audio only‚Ä¶).
Sauf que le transcodage, c‚Äôest **TR√àS** gourmand en ressources.
4 encodages ¬´¬†rapides¬†¬ª en parall√®le occupent sans probl√®me un i5-3450 √† 100% et les m√™mes en version standard un i7-8700K.
C√¥t√© serveur, un CPU capable d‚Äôencaisser √ßa commence avec la gamme [Xeon](https://www.intel.fr/content/www/fr/fr/products/processors/xeon.html) √† plus de 250‚Ç¨ pi√®ce, soit des machines en location √† plus de 100‚Ç¨/mois. Inaccessible pour du grand public‚Ä¶

# HLS¬†√† la rescousse¬†!

Du coup, on veut trouver un syst√®me de streaming capable d‚Äôutiliser les ressources du lieu de captation et √©vitant d‚Äôavoir un flux permanent ouvert √† la fois entre la captation et le serveur mais aussi entre le serveur et les clients. Et √ßa tombe bien, on a un format magique qui permet tout √ßa : [HLS](https://fr.wikipedia.org/wiki/HTTP_Live_Streaming).

√Ä la diff√©rence des fluxs pr√©c√©dents qui se basent directement sur TCP ou UDP, HLS repose sur le protocole [HTTP](https://fr.wikipedia.org/wiki/Hypertext_Transfer_Protocol).
Les vid√©os diffus√©es sont en fait d√©coup√©es en tas de petits morceaux [MPEG-TS](https://fr.wikipedia.org/wiki/MPEG_Transport_Stream) de dur√©e plus ou moins fixe (g√©n√©ralement quelques secondes), et le tout est servi de mani√®re classique par le serveur web.
La vid√©o compl√®te est reconstitu√©e par les clients √† partir d‚Äôun index [M3U](https://fr.wikipedia.org/wiki/M3U) tout aussi classique et simple, qui donne l‚Äôordre des morceaux.

```
#EXTM3U
#EXT-X-VERSION:3
#EXT-X-TARGETDURATION:6
#EXT-X-MEDIA-SEQUENCE:2350
#EXTINF:6.000000,
1080p/1566166537.ts
#EXTINF:4.000000,
1080p/1566166543.ts
#EXTINF:6.000000,
1080p/1566166547.ts
```

En bonus, le syst√®me est auto-adaptatif. Les fragments de toutes les r√©solutions √©tant disponibles, un client peut passer tranquillement d‚Äôune r√©solution √† l‚Äôautre.
Un autre index permet de d√©clarer les bandes passantes n√©cessaires pour chaque r√©solution, le client fera ses courses avec ce qu‚Äôil peut supporter.

```
#EXTM3U
#EXT-X-VERSION:3
#EXT-X-STREAM-INF:BANDWIDTH=800000,RESOLUTION=640x360
360p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=1400000,RESOLUTION=842x480
480p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=2800000,RESOLUTION=1280x720
720p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=5000000,RESOLUTION=1920x1080
1080p.m3u8
```

Aucune magie ni protocole compliqu√© pour la diffusion donc ! N‚Äôimporte quel serveur web, y compris un [Scaleway √† 6 centimes de l‚Äôheure](https://www.scaleway.com/en/virtual-instances/development/)‚Ä¶  

L‚Äôencodage peut aussi se faire simplement sur la machine locale.
On encode dans tous les formats souhait√©s, on envoie √ßa sur le serveur web par n‚Äôimporte quel moyen √† disposition (FTP, SSHFS, rsync, IPoAC‚Ä¶), et basta !

L‚Äôeffet Kiss-Cool continue, puisque le tout √©tant uniquement du HTTP classique, il est parfaitement possible de mettre en place de la r√©partition de charge tout aussi classiquement, avec du [HAProxy](https://www.haproxy.org/) ou du [DNS¬†tourniquet](https://fr.wikipedia.org/wiki/DNS_round-robin) ou des miroirs avec du Nginx en [proxy inverse](https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/) sur une source primaire avec [un peu de cache](https://docs.nginx.com/nginx/admin-guide/content-cache/content-caching/) pour soulager tout le monde.

Et √ßa continue encore et encore, parce que MPEG-TS √©tant du [H.264](https://fr.wikipedia.org/wiki/H.264), √ßa a le bon go√ªt d‚Äô√™tre [support√© nativement](https://caniuse.com/#feat=mpeg4) par les navigateurs¬†! On trouve m√™me [un lecteur clef-en-main](https://github.com/video-dev/hls.js/) √† int√©grer sur un site Internet, histoire de pouvoir se passer des gros fournisseurs centralis√©s privateurs habituels.

# OBS NDI

Il nous reste du coup un dernier probl√®me √† r√©soudre : dans le cas d‚Äôun streaming de jeu vid√©o par exemple, on veut g√©n√©ralement r√©aliser l‚Äôencodage sur une machine diff√©rente que celle de jeu. L‚Äôencodage √©tant un processus tr√®s consommateur, il est en effet assez peu envisageable d‚Äôutiliser massivement un CPU d√©j√† bien occup√© et qu‚Äôon aurait plut√¥t envie de le voir s‚Äôoccuper du rendu du jeu‚Ä¶

G√©n√©ralement, l‚Äôoutil utilis√© par les diffuseurs est [OBS](https://obsproject.com/fr/). Probl√®me, par d√©faut il ne propose que des options n√©cessitant un encodage de la captation pour l‚Äôenvoyer vers des serveurs de stream conventionnels (Twitch, Youtube, Periscope‚Ä¶) ou vers un ffmpeg distant (flux, multicast, tout √ßa tout √ßa‚Ä¶).

Heureusement, gr√¢ce √† [Palakis](https://twitter.com/LePalakis) il existe une extension [OBS-NDI](https://github.com/Palakis/obs-ndi) utilisant le protocole (propri√©taire‚Ä¶ üò≠) [NDI](https://fr.newtek.com/ndi/). NDI a beau √™tre propri√©taire, son concepteur fournit [un kit de d√©veloppement](https://fr.newtek.com/ndi/sdk/) y compris pour GNU/Linux. Ce protocole se base sur [Avahi/Zeroconf](https://fr.wikipedia.org/wiki/Avahi_(logiciel)) pour la d√©tection des sources et encha√Æne ensuite avec des connexions TCP standard. Ceci permet l‚Äôenvoi direct (sans encodage) du flux capt√© vers tout syst√®me causant NDI. Et √ßa tombe bien, ffmpeg supporte NDI [depuis sa version 3.4](https://www.newtek.com/blog/ffmpeg-3-4-adds-ndi-io/).

Un avantage notable de NDI est de ne pas n√©cessiter un ffmpeg permanent √† tourner en face. On lance OBS quand on veut, et c‚Äôest la machine en face qui s‚Äôy connectera quand elle souhaitera. On peut ainsi relancer le ffmpeg sans n√©cessiter d‚Äôintervention sur l‚ÄôOBS, par exemple en cas de probl√®me ou pour changer les param√®tres d‚Äôencodage.

√Ä noter que cons√©quence de l‚Äôusage de Avahi, NDI est [un protocole r√©seau passant difficilement les pare-feu](https://fr.wikipedia.org/wiki/Protocole_r%C3%A9seau_passant_difficilement_les_pare-feu). En effet, apr√®s le requ√©tage Zeroconf, [un port r√©seau √©ph√©m√®re](https://support.newtek.com/hc/en-us/articles/218109497-NDI-Video-Data-Flow) est ouvert pour chaque nouveau client qui arrive. Le premier client passera par le port 49152, le second par le 49153, etc. Veillez donc √† d√©sactiver vos pare-feu si vous utilisez ce protocole ou au moins d‚Äôouvrir quelques ports √† partir de 49152.

# Mettons tout √ßa ensemble

J‚Äôai fais le choix de Arch Linux pour monter une machine de stream, tout simplement parce que [les d√©p√¥ts AUR](https://wiki.archlinux.fr/AUR_4.0.0) contiennent tout ce qu‚Äôil faut pour OBS, NDI & ffmpeg, contrairement √† d‚Äôautres distributions o√π il sera n√©cessaire de beaucoup bidouiller pour tout faire fonctionner.

* [SDK NDI](https://aur.archlinux.org/packages/ndi-sdk/)
* [OBS NDI](https://aur.archlinux.org/packages/ndi-sdk/)
* [OBS](https://www.archlinux.org/packages/community/x86_64/obs-studio/)

Il ne manque qu‚Äôun ffmpeg construit pour NDI, mais on peut facilement le compiler √† la main une fois le SDK NDI install√©¬†:

```
git clone https://git.ffmpeg.org/ffmpeg.git --branch n4.2 --depth=1
cd ./ffmpeg
./configure --enable-gpl --enable-nonfree --enable-libndi_newtek --enable-libx264 --cpu=native
make -j8
```

Les invocations ffmpeg relevants plus du chamanisme que d‚Äôautre chose, j‚Äôai simplifi√© le boulot dans quelques scripts¬†:

* [encode.rb](https://git.imirhil.fr/aeris/streaming/src/branch/master/encode.rb)¬†: un script consommant en entr√©e un flux NDI et l‚Äôencodant dans toutes les r√©solutions souhait√©es, actuellement HLS 1080p@30 + 720p@30 +¬†480p@30 + 320p@30 + audio only, FLV 1080p@30 pour Twitch et un stockage MPEG4 1080p@30 sur disque.
* [rsync.py](https://git.imirhil.fr/aeris/streaming/src/branch/master/rsync.py) : un script un peu plus malin qu‚Äôun rsync global de tous les flux HLS. Il priorise d‚Äôabord les fragments `.ts avant d‚Äôenvoyer le `.m3u8`, de sorte √† ne pas publier un index dont certains bouts ne sont pas
  encore publi√©s, et le tout avec un processus `rsync` par r√©solution plut√¥t que d‚Äôattendre que tout 1080p soit envoy√© avant d‚Äôattaquer le 720p.

 Une diffusion se r√©sume √† lancer `./encode.rb` (les 4 encodages HLS), `./encode.rb live` (pour le stockage et Twitch) et `./rsync.py` (pour l‚Äôenvoi sur le serveur).

La configuration du serveur nginx est disponible [ici](https://git.imirhil.fr/aeris/streaming/src/branch/master/nginx-upstream.conf), celle pour un √©ventuel miroir-cache [l√†](https://git.imirhil.fr/aeris/streaming/src/branch/master/nginx-upstream.conf) et l‚Äôindex m3u8 global [ici](https://git.imirhil.fr/aeris/streaming/src/branch/master/index.m3u8).

Ces scripts sont bas√©s en grosse partie sur des id√©es de [Benjamin Sonntag](https://mamot.fr/@vincib) qui utilise ce genre de m√©thode pour les solutions de streaming de [Octopuce](https://www.octopuce.fr/). Ils sont largement am√©liorables, en particulier pour sortir quelques param√®tres un peu trop en dur √† mon go√ªt (l‚Äôadresse du flux NDI, l‚Äôadresse du serveur SSH‚Ä¶).

Je m‚Äôen sers r√©guli√®rement pour les diffusions de [mon chaton](https://mamot.fr/@nlavielle), la machine d‚Äôencodage √©tait un simple PC portable [W251HUQ](http://clevo-europe.com/default_zone/fr/html/Prod_Notebook_EpureS4_More.php) de chez Clevo, √©quip√© d‚Äôun i5-2520M. En bande passante, comptez minimum 20Mbps, en dessous vous ne tiendrez pas la charge. C‚Äôest aussi ce syst√®me qui a assur√© la diffusion de [PSES 2018](https://passageenseine.fr/), sur un i7-8700K cette fois, et avec une carte d‚Äôacquisition [BlackMagic DeckLink Mini Recorder](https://www.blackmagicdesign.com/fr/products/decklink/techspecs/W-DLK-06) (tr√®s [GNU](https://aur.archlinux.org/packages/decklink/) & [ffmpeg](https://www.ffmpeg.org/ffmpeg-devices.html#decklink) compatible aussi).

R√©sumons donc. On a OBS qui s‚Äôoccupe de la captation. Qui transmet tout √ßa via NDI vers un ffmpeg qui tourne sur une autre machine. ffmpeg en charge de l‚Äôencodage dans les formats souhait√©s. Un bon vieux [rsync](https://rsync.samba.org/) des familles pour envoyer les morceaux & index HLS vers un serveur web. Et enfin un nginx qui sert tout √ßa classiquement aux visiteurs.

 Et voil√† ! üòä
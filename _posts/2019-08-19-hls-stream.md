---
title: Streaming HLS
---

DÃ©jÃ  2 ans sans article de blogâ€¦ On va corriger Ã§a du coup !

Aujourdâ€™hui, on va parler streaming, et surtout comment en faire avec (presque) que du logiciel libre et en tout cas en Ã©vitant au maximum les outils habituels privateurs et en proposant des alternatives auto-hÃ©bergeables et dÃ©centralisÃ©es pour tous !

# Les problÃ¨mes du streaming Â«Â standardÂ Â»

Le streaming Â«Â Ã  lâ€™ancienneÂ Â» est gÃ©nÃ©ralement basÃ© sur un flux TCP continu, type [RTP](https://fr.wikipedia.org/wiki/Real-time_Transport_Protocol), [RTSP](https://fr.wikipedia.org/wiki/Real_Time_Streaming_Protocol) ou [RTMP](https://fr.wikipedia.org/wiki/Real_Time_Messaging_Protocol).

Ces formats fonctionnent plutÃ´t bien tant quâ€™on a Ã  disposition du matÃ©riel professionnel de diffusion, mais câ€™est gÃ©nÃ©ralement la croix et la banniÃ¨re quand on veut faire avec du matÃ©riel classique comme on en a partout Ã  la maison.

Le principal problÃ¨me de ce type de flux est quâ€™il sâ€™agit justement dâ€™un fluxÂ ! Il faut une connexion permanente et stable entre le lieu de la captation et le serveur de streaming. Au moindre glitch rÃ©seau, gÃ©nÃ©ralement tout plante et il faut tout relancer, et dans le bon ordre.
Ã‡a suppose aussi des choses assez exotiques, type du [multicast](https://fr.wikipedia.org/wiki/Multicast) comme on en trouve dans [la documentation officielle](https://trac.ffmpeg.org/wiki/StreamingGuide) de [ffmpeg](https://ffmpeg.org/), qui nÃ©cessitent en plus du matÃ©riel rÃ©seau compatible.
Idem cÃ´tÃ© client, chaque client va devoir maintenir un flux constant ouvert, ce qui engorge rapidement le serveur. Et parler de cache ou de miroir sur du flux relÃ¨ve bien entendu de lâ€™exploit voire de lâ€™impossibilitÃ©â€¦ 

Lâ€™autre problÃ¨me, câ€™est quâ€™on trouve tout plein de solutions qui reposent sur un encodage des vidÃ©os au niveau du serveur en face et non de lÃ  oÃ¹ est effectuÃ©e la captation. Câ€™est par exemple le cas avec [le module RTMP pour Nginx](https://github.com/arut/nginx-rtmp-module), qui prend un unique flux en entrÃ©e (gÃ©nÃ©ralement du 1080p) et le transcode dans les autres rÃ©solutions (720p, 480p, 320p, audio onlyâ€¦).
Sauf que le transcodage, câ€™est **TRÃˆS** gourmand en ressources.
4 encodages Â«Â rapidesÂ Â» en parallÃ¨le occupent sans problÃ¨me un i5-3450 Ã  100% et les mÃªmes en version standard un i7-8700K.
CÃ´tÃ© serveur, un CPU capable dâ€™encaisser Ã§a commence avec la gamme [Xeon](https://www.intel.fr/content/www/fr/fr/products/processors/xeon.html) Ã  plus de 250â‚¬ piÃ¨ce, soit des machines en location Ã  plus de 100â‚¬/mois. Inaccessible pour du grand publicâ€¦

# HLSÂ Ã  la rescousseÂ !

Du coup, on veut trouver un systÃ¨me de streaming capable dâ€™utiliser les ressources du lieu de captation et Ã©vitant dâ€™avoir un flux permanent ouvert Ã  la fois entre la captation et le serveur mais aussi entre le serveur et les clients. Et Ã§a tombe bien, on a un format magique qui permet tout Ã§a : [HLS](https://fr.wikipedia.org/wiki/HTTP_Live_Streaming).

Ã€ la diffÃ©rence des fluxs prÃ©cÃ©dents qui se basent directement sur TCP ou UDP, HLS repose sur le protocole [HTTP](https://fr.wikipedia.org/wiki/Hypertext_Transfer_Protocol).
Les vidÃ©os diffusÃ©es sont en fait dÃ©coupÃ©es en tas de petits morceaux [MPEG-TS](https://fr.wikipedia.org/wiki/MPEG_Transport_Stream) de durÃ©e plus ou moins fixe (gÃ©nÃ©ralement quelques secondes), et le tout est servi de maniÃ¨re classique par le serveur web.
La vidÃ©o complÃ¨te est reconstituÃ©e par les clients Ã  partir dâ€™un index [M3U](https://fr.wikipedia.org/wiki/M3U) tout aussi classique et simple, qui donne lâ€™ordre des morceaux.

```
#EXTM3U
#EXT-X-VERSION:3
#EXT-X-TARGETDURATION:6
#EXT-X-MEDIA-SEQUENCE:2350
#EXTINF:6.000000,
1080p/1566166537.ts
#EXTINF:4.000000,
1080p/1566166543.ts
#EXTINF:6.000000,
1080p/1566166547.ts
```

En bonus, le systÃ¨me est auto-adaptatif. Les fragments de toutes les rÃ©solutions Ã©tant disponibles, un client peut passer tranquillement dâ€™une rÃ©solution Ã  lâ€™autre.
Un autre index permet de dÃ©clarer les bandes passantes nÃ©cessaires pour chaque rÃ©solution, le client fera ses courses avec ce quâ€™il peut supporter.

```
#EXTM3U
#EXT-X-VERSION:3
#EXT-X-STREAM-INF:BANDWIDTH=800000,RESOLUTION=640x360
360p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=1400000,RESOLUTION=842x480
480p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=2800000,RESOLUTION=1280x720
720p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=5000000,RESOLUTION=1920x1080
1080p.m3u8
```

Aucune magie ni protocole compliquÃ© pour la diffusion donc ! Nâ€™importe quel serveur web, y compris un [Scaleway Ã  6 centimes de lâ€™heure](https://www.scaleway.com/en/virtual-instances/development/)â€¦  

Lâ€™encodage peut aussi se faire simplement sur la machine locale.
On encode dans tous les formats souhaitÃ©s, on envoie Ã§a sur le serveur web par nâ€™importe quel moyen Ã  disposition (FTP, SSHFS, rsync, IPoACâ€¦), et basta !

Lâ€™effet Kiss-Cool continue, puisque le tout Ã©tant uniquement du HTTP classique, il est parfaitement possible de mettre en place de la rÃ©partition de charge tout aussi classiquement, avec du [HAProxy](https://www.haproxy.org/) ou du [DNSÂ tourniquet](https://fr.wikipedia.org/wiki/DNS_round-robin) ou des miroirs avec du Nginx en [proxy inverse](https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/) sur une source primaire avec [un peu de cache](https://docs.nginx.com/nginx/admin-guide/content-cache/content-caching/) pour soulager tout le monde.

Et Ã§a continue encore et encore, parce que MPEG-TS Ã©tant du [H.264](https://fr.wikipedia.org/wiki/H.264), Ã§a a le bon goÃ»t dâ€™Ãªtre [supportÃ© nativement](https://caniuse.com/#feat=mpeg4) par les navigateursÂ ! On trouve mÃªme [un lecteur clef-en-main](https://github.com/video-dev/hls.js/) Ã  intÃ©grer sur un site Internet, histoire de pouvoir se passer des gros fournisseurs centralisÃ©s privateurs habituels.

# OBS NDI

Il nous reste du coup un dernier problÃ¨me Ã  rÃ©soudre : dans le cas dâ€™un streaming de jeu vidÃ©o par exemple, on veut gÃ©nÃ©ralement rÃ©aliser lâ€™encodage sur une machine diffÃ©rente que celle de jeu. Lâ€™encodage Ã©tant un processus trÃ¨s consommateur, il est en effet assez peu envisageable dâ€™utiliser massivement un CPU dÃ©jÃ  bien occupÃ© et quâ€™on aurait plutÃ´t envie de le voir sâ€™occuper du rendu du jeuâ€¦

GÃ©nÃ©ralement, lâ€™outil utilisÃ© par les diffuseurs est [OBS](https://obsproject.com/fr/). ProblÃ¨me, par dÃ©faut il ne propose que des options nÃ©cessitant un encodage de la captation pour lâ€™envoyer vers des serveurs de stream conventionnels (Twitch, Youtube, Periscopeâ€¦) ou vers un ffmpeg distant (flux, multicast, tout Ã§a tout Ã§aâ€¦).

Heureusement, grÃ¢ce Ã  [Palakis](https://twitter.com/LePalakis) il existe une extension [OBS-NDI](https://github.com/Palakis/obs-ndi) utilisant le protocole (propriÃ©taireâ€¦ ğŸ˜­) [NDI](https://fr.newtek.com/ndi/). NDI a beau Ãªtre propriÃ©taire, son concepteur fournit [un kit de dÃ©veloppement](https://fr.newtek.com/ndi/sdk/) y compris pour GNU/Linux. Ce protocole se base sur [Avahi/Zeroconf](https://fr.wikipedia.org/wiki/Avahi_(logiciel)) pour la dÃ©tection des sources et enchaÃ®ne ensuite avec des connexions TCP standard. Ceci permet lâ€™envoi direct (sans encodage) du flux captÃ© vers tout systÃ¨me causant NDI. Et Ã§a tombe bien, ffmpeg supporte NDI [depuis sa version 3.4](https://www.newtek.com/blog/ffmpeg-3-4-adds-ndi-io/).

Un avantage notable de NDI est de ne pas nÃ©cessiter un ffmpeg permanent Ã  tourner en face. On lance OBS quand on veut, et câ€™est la machine en face qui sâ€™y connectera quand elle souhaitera. On peut ainsi relancer le ffmpeg sans nÃ©cessiter dâ€™intervention sur lâ€™OBS, par exemple en cas de problÃ¨me ou pour changer les paramÃ¨tres dâ€™encodage.

Ã€ noter que consÃ©quence de lâ€™usage de Avahi, NDI est [un protocole rÃ©seau passant difficilement les pare-feu](https://fr.wikipedia.org/wiki/Protocole_r%C3%A9seau_passant_difficilement_les_pare-feu). En effet, aprÃ¨s le requÃ©tage Zeroconf, [un port rÃ©seau Ã©phÃ©mÃ¨re](https://support.newtek.com/hc/en-us/articles/218109497-NDI-Video-Data-Flow) est ouvert pour chaque nouveau client qui arrive. Le premier client passera par le port 49152, le second par le 49153, etc. Veillez donc Ã  dÃ©sactiver vos pare-feu si vous utilisez ce protocole ou au moins dâ€™ouvrir quelques ports Ã  partir de 49152.

# Mettons tout Ã§a ensemble

Jâ€™ai fais le choix de Arch Linux pour monter une machine de stream, tout simplement parce que [les dÃ©pÃ´ts AUR](https://wiki.archlinux.fr/AUR_4.0.0) contiennent tout ce quâ€™il faut pour OBS, NDI & ffmpeg, contrairement Ã  dâ€™autres distributions oÃ¹ il sera nÃ©cessaire de beaucoup bidouiller pour tout faire fonctionner.

* [SDK NDI](https://aur.archlinux.org/packages/ndi-sdk/)
* [OBS NDI](https://aur.archlinux.org/packages/ndi-sdk/)
* [OBS](https://www.archlinux.org/packages/community/x86_64/obs-studio/)

Il ne manque quâ€™un ffmpeg construit pour NDI, mais on peut facilement le compiler Ã  la main une fois le SDK NDI installÃ©Â :

```
git clone https://git.ffmpeg.org/ffmpeg.git --branch n4.2 --depth=1
cd ./ffmpeg
./configure --enable-gpl --enable-nonfree --enable-libndi_newtek --enable-libx264 --cpu=native
make -j8
```

Les invocations ffmpeg relevants plus du chamanisme que dâ€™autre chose, jâ€™ai simplifiÃ© le boulot dans quelques scriptsÂ :

* [encode.rb](https://git.imirhil.fr/aeris/streaming/src/branch/master/encode.rb)Â : un script consommant en entrÃ©e un flux NDI et lâ€™encodant dans toutes les rÃ©solutions souhaitÃ©es, actuellement HLS 1080p@30 + 720p@30 +Â 480p@30 + 320p@30 + audio only, FLV 1080p@30 pour Twitch et un stockage MPEG4 1080p@30 sur disque.
* [rsync.py](https://git.imirhil.fr/aeris/streaming/src/branch/master/rsync.py) : un script un peu plus malin quâ€™un rsync global de tous les flux HLS. Il priorise dâ€™abord les fragments `.ts avant dâ€™envoyer le `.m3u8`, de sorte Ã  ne pas publier un index dont certains bouts ne sont pas
  encore publiÃ©s, et le tout avec un processus `rsync` par rÃ©solution plutÃ´t que dâ€™attendre que tout 1080p soit envoyÃ© avant dâ€™attaquer le 720p.

 Une diffusion se rÃ©sume Ã  lancer `./encode.rb` (les 4 encodages HLS), `./encode.rb live` (pour le stockage et Twitch) et `./rsync.py` (pour lâ€™envoi sur le serveur).

La configuration du serveur nginx est disponible [ici](https://git.imirhil.fr/aeris/streaming/src/branch/master/nginx-upstream.conf), celle pour un Ã©ventuel miroir-cache [lÃ ](https://git.imirhil.fr/aeris/streaming/src/branch/master/nginx-upstream.conf) et lâ€™index m3u8 global [ici](https://git.imirhil.fr/aeris/streaming/src/branch/master/index.m3u8).

Ces scripts sont basÃ©s en grosse partie sur des idÃ©es de [Benjamin Sonntag](https://mamot.fr/@vincib) qui utilise ce genre de mÃ©thode pour les solutions de streaming de [Octopuce](https://www.octopuce.fr/). Ils sont largement amÃ©liorables, en particulier pour sortir quelques paramÃ¨tres un peu trop en dur Ã  mon goÃ»t (lâ€™adresse du flux NDI, lâ€™adresse du serveur SSHâ€¦).

Je mâ€™en sers rÃ©guliÃ¨rement pour les diffusions de [mon chaton](https://mamot.fr/@nlavielle), la machine dâ€™encodage Ã©tait un simple PC portable [W251HUQ](http://clevo-europe.com/default_zone/fr/html/Prod_Notebook_EpureS4_More.php) de chez Clevo, Ã©quipÃ© dâ€™un i5-2520M. En bande passante, comptez minimum 20Mbps, en dessous vous ne tiendrez pas la charge. Câ€™est aussi ce systÃ¨me qui a assurÃ© la diffusion de [PSES 2018](https://passageenseine.fr/), sur un i7-8700K cette fois, et avec une carte dâ€™acquisition [BlackMagic DeckLink Mini Recorder](https://www.blackmagicdesign.com/fr/products/decklink/techspecs/W-DLK-06) (trÃ¨s [GNU](https://aur.archlinux.org/packages/decklink/) & [ffmpeg](https://www.ffmpeg.org/ffmpeg-devices.html#decklink) compatible aussi).

RÃ©sumons donc. On a OBS qui sâ€™occupe de la captation. Qui transmet tout Ã§a via NDI vers un ffmpeg qui tourne sur une autre machine. ffmpeg en charge de lâ€™encodage dans les formats souhaitÃ©s. Un bon vieux [rsync](https://rsync.samba.org/) des familles pour envoyer les morceaux & index HLS vers un serveur web. Et enfin un nginx qui sert tout Ã§a classiquement aux visiteurs.

 Et voilÃ  ! ğŸ˜Š